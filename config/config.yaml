# ML Pipeline Configuration

# Data Collection Settings
data:
  # Stock symbols to analyze
  symbols:
    - AAPL
    - MSFT
    - GOOGL
    - AMZN
  # Date range
  start_date: "2020-01-01"
  end_date: "2024-12-31"
  # Data interval (1d, 1wk, 1mo)
  interval: "1d"

# Feature Engineering Settings
features:
  # Technical indicators to calculate
  indicators:
    - SMA_20
    - SMA_50
    - RSI
    - MACD
    - BB_upper
    - BB_lower
  # Lookback period for features
  lookback_period: 60

# Model Training Settings
models:
  # Models to train and compare
  algorithms:
    - random_forest
    - xgboost
    - lstm
  
  # Train-test split ratio
  test_size: 0.2
  
  # Random Forest parameters
  random_forest:
    n_estimators: 100
    max_depth: 10
    random_state: 42
  
  # XGBoost parameters
  xgboost:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
    random_state: 42
  
  # LSTM parameters
  lstm:
    epochs: 50
    batch_size: 32
    units: 50
    dropout: 0.2

# Trading Strategy Settings
strategy:
  # Buy/Sell/Hold thresholds
  buy_threshold: 0.6
  sell_threshold: 0.4
  # Initial capital
  initial_capital: 100000
  # Commission rate
  commission: 0.001

# Backtesting Settings
backtest:
  # Portfolio settings
  freq: "1D"
  # Risk metrics
  risk_free_rate: 0.02

# Reinforcement Learning Settings
reinforcement_learning:
  # RL algorithms to train
  algorithms:
    - ppo
    - a2c
    - ddpg
    - td3
    - sac
  
  # Environment settings
  environment:
    initial_balance: 100000
    commission: 0.001
    lookback_window: 60
    reward_function: "sharpe"  # Options: profit, sharpe, sortino, cvar, max_drawdown
  
  # PPO (Proximal Policy Optimization) parameters
  ppo:
    total_timesteps: 200000
    learning_rate: 0.0002
    n_steps: 2048
    batch_size: 128
    gamma: 0.99
    gae_lambda: 0.95 # LOWERED: Focuses on shorter-term volatility (better for active trading)
    ent_coef: 0.05 # INCREASED: Forces exploration (prevents getting stuck in Buy&Hold)
  
  # A2C (Advantage Actor-Critic) parameters (Faster, but often less stable than PPO)
  a2c:
    total_timesteps: 200000
    learning_rate: 0.0003
    n_steps: 20              # Increased: Allows looking further ahead before bootstrap
    gamma: 0.99
    ent_coef: 0.01
    max_grad_norm: 0.5       # Prevents exploding gradients
  
  # DDPG (Deep Deterministic Policy Gradient) parameters (Hard to tune for stocks - prone to divergence)
  ddpg:
    total_timesteps: 150000
    learning_rate: 0.0001    # Needs to be very slow
    buffer_size: 100000
    batch_size: 128
    gamma: 0.99
    tau: 0.005

  # TD3 (Twin Delayed DDPG) parameters (Better version of DDPG)
  td3:
    total_timesteps: 150000
    learning_rate: 0.0003
    buffer_size: 100000
    batch_size: 256
    gamma: 0.99
    tau: 0.005
    policy_delay: 2          # Updates policy less frequently than Q-function (stability)
  
  # SAC (Soft Actor-Critic) parameters (Best Off-Policy Choice for Stocks)
  sac:
    total_timesteps: 150000
    learning_rate: 0.0003
    buffer_size: 100000
    batch_size: 256          # SAC loves large batches
    gamma: 0.99
    tau: 0.005
    ent_coef: "auto"         # Automatically adjusts exploration
  
  # DQN (Deep Q-Network) parameters - requires discrete actions
  dqn:
    total_timesteps: 200000
    learning_rate: 0.0001
    buffer_size: 100000
    batch_size: 64
    gamma: 0.99
    exploration_fraction: 0.3 # Spend first 30% of time exploring randomly
    exploration_final_eps: 0.05
    target_update_interval: 1000 # Stabilizes training
  
  # Test split ratio
  test_size: 0.2

# Time-series Forecasting Settings
time_series:
  # Algorithms to train
  algorithms:
    - prophet
    - arima
    - garch
    - neuralprophet
    - naive
  
  # Data frequency
  freq: "D"
  
  # Test split ratio
  test_size: 0.2
  
  # Prophet parameters
  prophet:
    seasonality_mode: "additive"
    changepoint_prior_scale: 0.05
    yearly_seasonality: true
    weekly_seasonality: true
    daily_seasonality: false
  
  # ARIMA parameters
  arima:
    order: [1, 1, 1]
  
  # GARCH parameters
  garch:
    p: 1
    q: 1
    dist: "normal"
  
  # NeuralProphet parameters
  neuralprophet:
    n_changepoints: 5
    yearly_seasonality: true
    weekly_seasonality: true
    daily_seasonality: false

# Deep Learning Time-series Settings
deep_learning:
  # Algorithms to train
  algorithms:
    - mlp
    - lstm
    - bilstm
    - lstm_attention
    - gru
    - cnn_lstm
    - stacked_lstm
  
  # Lookback window for sequences
  lookback: 60
  
  # Test split ratio
  test_size: 0.2
  
  # MLP parameters
  mlp:
    units: [128, 64, 32]
    dropout: 0.2
    epochs: 50
    batch_size: 32
  
  # LSTM parameters
  lstm:
    units: 50
    dropout: 0.2
    epochs: 50
    batch_size: 32
  
  # Bidirectional LSTM parameters
  bilstm:
    units: 50
    dropout: 0.2
    epochs: 50
    batch_size: 32
  
  # LSTM with Attention parameters
  lstm_attention:
    units: 50
    dropout: 0.2
    epochs: 50
    batch_size: 32
  
  # GRU parameters
  gru:
    units: 50
    dropout: 0.2
    epochs: 50
    batch_size: 32
  
  # CNN-LSTM parameters
  cnn_lstm:
    cnn_filters: 32
    lstm_units: 50
    dropout: 0.2
    epochs: 50
    batch_size: 32
  
  # Stacked LSTM parameters
  stacked_lstm:
    units: [50, 50]
    dropout: 0.2
    epochs: 50
    batch_size: 32
